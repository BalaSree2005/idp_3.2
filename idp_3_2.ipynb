{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW5Uz+bz/hn62LUcJ0PkDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BalaSree2005/idp_3.2/blob/main/idp_3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Giph5Ud6WpRb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paths to the datasets\n",
        "train_path = '/content/drive/MyDrive/parkinson/Dataset/train'\n",
        "valid_path = '/content/drive/MyDrive/parkinson/Dataset/valid'\n",
        "test_path = '/content/drive/MyDrive/parkinson/Dataset/test'\n",
        "\n",
        "# Custom Dataset Class\n",
        "class ParkinsonDataset(Dataset):\n",
        "    def __init__(self, directory):\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        for label, subdir in enumerate(os.listdir(directory)):\n",
        "            subdir_path = os.path.join(directory, subdir)\n",
        "            if os.path.isdir(subdir_path):\n",
        "                for img_name in os.listdir(subdir_path):\n",
        "                    if img_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                        img_path = os.path.join(subdir_path, img_name)\n",
        "                        img = Image.open(img_path).resize((150, 150))  # Resize to 150x150\n",
        "                        self.images.append(np.array(img) / 255.0)  # Normalize\n",
        "                        self.labels.append(label)\n",
        "\n",
        "        self.images = np.array(self.images).astype(np.float32)\n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.tensor(self.images[idx].flatten(), dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return image, label\n",
        "\n",
        "# Load data using DataLoader with larger batch size\n",
        "batch_size = 128  # Increased batch size for better GPU utilization\n",
        "train_dataset = ParkinsonDataset(train_path)\n",
        "valid_dataset = ParkinsonDataset(valid_path)\n",
        "test_dataset = ParkinsonDataset(test_path)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the neural network model\n",
        "class ParkinsonNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ParkinsonNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(150 * 150 * 3, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 2)  # Assuming 2 classes (0 and 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = ParkinsonNet().to(device)\n",
        "\n",
        "# Enable DataParallel if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scaler = torch.cuda.amp.GradScaler()  # Enable Mixed Precision Training\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, criterion, optimizer, train_loader, valid_loader, epochs=10):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision training\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valid_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_losses.append(val_loss / len(valid_loader))\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Training completed in {total_time:.2f} seconds\")\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Neural Network...\")\n",
        "train_losses, val_losses = train_model(model, criterion, optimizer, train_loader, valid_loader, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, axis=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy, all_preds, all_labels\n",
        "\n",
        "test_accuracy, test_preds, test_labels = evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix and Classification Report\n",
        "test_cm = confusion_matrix(test_labels, test_preds)\n",
        "print(\"Test Confusion Matrix:\")\n",
        "print(test_cm)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, test_preds))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(test_labels), yticklabels=np.unique(test_labels))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Test Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Set device (CPU only)\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paths to the datasets\n",
        "train_path = '/content/drive/MyDrive/parkinson/Dataset/train'\n",
        "valid_path = '/content/drive/MyDrive/parkinson/Dataset/valid'\n",
        "test_path = '/content/drive/MyDrive/parkinson/Dataset/test'\n",
        "\n",
        "# Custom Dataset Class\n",
        "class ParkinsonDataset(Dataset):\n",
        "    def __init__(self, directory):\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        for label, subdir in enumerate(os.listdir(directory)):\n",
        "            subdir_path = os.path.join(directory, subdir)\n",
        "            if os.path.isdir(subdir_path):\n",
        "                for img_name in os.listdir(subdir_path):\n",
        "                    if img_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                        img_path = os.path.join(subdir_path, img_name)\n",
        "                        img = Image.open(img_path).resize((150, 150))  # Resize to 150x150\n",
        "                        self.images.append(np.array(img) / 255.0)  # Normalize\n",
        "                        self.labels.append(label)\n",
        "\n",
        "        self.images = np.array(self.images).astype(np.float32)\n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.tensor(self.images[idx].flatten(), dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return image, label\n",
        "\n",
        "# Load data using DataLoader\n",
        "batch_size = 128\n",
        "train_dataset = ParkinsonDataset(train_path)\n",
        "valid_dataset = ParkinsonDataset(valid_path)\n",
        "test_dataset = ParkinsonDataset(test_path)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Define the neural network model\n",
        "class ParkinsonNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ParkinsonNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(150 * 150 * 3, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 2)  # Assuming 2 classes (0 and 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = ParkinsonNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, criterion, optimizer, train_loader, valid_loader, epochs=10):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valid_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_losses.append(val_loss / len(valid_loader))\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Training completed in {total_time:.2f} seconds\")\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Neural Network...\")\n",
        "train_losses, val_losses = train_model(model, criterion, optimizer, train_loader, valid_loader, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, axis=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy, all_preds, all_labels\n",
        "\n",
        "test_accuracy, test_preds, test_labels = evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix and Classification Report\n",
        "test_cm = confusion_matrix(test_labels, test_preds)\n",
        "print(\"Test Confusion Matrix:\")\n",
        "print(test_cm)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, test_preds))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(test_labels), yticklabels=np.unique(test_labels))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Test Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bPbSCgdgW5nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import os\n",
        "import multiprocessing\n",
        "\n",
        "# Check GPU Availability\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    print(f\"Number of GPUs Available: {num_gpus}\")\n",
        "    for i in range(num_gpus):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPUs available, running on CPU.\")\n",
        "\n",
        "# Check Number of CPU Cores\n",
        "num_cpu_cores = multiprocessing.cpu_count()\n",
        "print(f\"Number of CPU Cores Available: {num_cpu_cores}\")\n",
        "\n",
        "# Check Parallelism using Batch Size Performance\n",
        "def measure_time(batch_size):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Dummy Tensor Computation to Simulate Training\n",
        "    x = torch.randn(batch_size, 150 * 150 * 3).to(\"cuda\" if num_gpus > 0 else \"cpu\")\n",
        "    model = torch.nn.Linear(150 * 150 * 3, 128).to(\"cuda\" if num_gpus > 0 else \"cpu\")\n",
        "\n",
        "    for _ in range(10):  # Simulate 10 forward passes\n",
        "        y = model(x)\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "# Measure time for different batch sizes\n",
        "batch_sizes = [32, 64, 128, 256]\n",
        "for batch in batch_sizes:\n",
        "    elapsed_time = measure_time(batch)\n",
        "    print(f\"Batch Size: {batch}, Time Taken: {elapsed_time:.4f} sec\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_fzFq_CGW8wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torchinfo import summary\n",
        "\n",
        "# Define the ParkinsonNet Model\n",
        "class ParkinsonNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ParkinsonNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(150 * 150 * 3, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 2)  # Assuming 2 classes (0 and 1)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ParkinsonNet().to(device)\n",
        "\n",
        "# 1. Display Model Summary (FLOPs, Parameters)\n",
        "print(\"\\n==== Model Summary ====\")\n",
        "summary(model, input_size=(1, 150 * 150 * 3))\n",
        "\n",
        "# 2. Measure Execution Time and Parallel Instructions\n",
        "def measure_parallel_execution(model, batch_size, input_size):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    dummy_input = torch.randn(batch_size, input_size).to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(dummy_input)\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    num_operations = 150 * 150 * 3 * batch_size  # Estimate based on input size\n",
        "\n",
        "    print(f\"Batch Size: {batch_size}, Time Taken: {elapsed_time:.6f} sec, Estimated Instructions: {num_operations / elapsed_time:.2e} instr/sec\")\n",
        "    return elapsed_time, num_operations\n",
        "\n",
        "# Test with Different Batch Sizes\n",
        "print(\"\\n==== Parallel Execution Analysis ====\")\n",
        "batch_sizes = [1, 32, 64, 128, 256]\n",
        "for batch_size in batch_sizes:\n",
        "    measure_parallel_execution(model, batch_size, 150 * 150 * 3)\n"
      ],
      "metadata": {
        "id": "6XDhvMxDXB2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torchinfo import summary\n",
        "\n",
        "# Force CPU Execution\n",
        "device = \"cpu\"\n",
        "\n",
        "# Define the ParkinsonNet Model\n",
        "class ParkinsonNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ParkinsonNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(150 * 150 * 3, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 2)  # Assuming 2 classes (0 and 1)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Model on CPU\n",
        "model = ParkinsonNet().to(device)\n",
        "\n",
        "# 1. Display Model Summary\n",
        "print(\"\\n==== Model Summary ====\")\n",
        "summary(model, input_size=(1, 150 * 150 * 3), device=\"cpu\")\n",
        "\n",
        "# 2. Measure Execution Time and Instructions Per Second on CPU\n",
        "def measure_cpu_execution(model, batch_size, input_size):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    dummy_input = torch.randn(batch_size, input_size).to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(dummy_input)\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    num_operations = 150 * 150 * 3 * batch_size  # Estimate based on input size\n",
        "\n",
        "    ips = num_operations / elapsed_time if elapsed_time > 0 else 0  # Instructions per second\n",
        "    print(f\"Batch Size: {batch_size}, Time Taken: {elapsed_time:.6f} sec, Estimated IPS: {ips:.2e} instr/sec\")\n",
        "    return elapsed_time, ips\n",
        "\n",
        "# Test with Different Batch Sizes on CPU\n",
        "print(\"\\n==== CPU Execution Analysis ====\")\n",
        "batch_sizes = [1, 32, 64, 128, 256]\n",
        "for batch_size in batch_sizes:\n",
        "    measure_cpu_execution(model, batch_size, 150 * 150 * 3)\n"
      ],
      "metadata": {
        "id": "M80STAiCXIAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "# Sample input tensor\n",
        "sample_input = torch.randn(1, 150 * 150 * 3).to(device)\n",
        "\n",
        "# Forward pass to get model output\n",
        "output = model(sample_input)\n",
        "\n",
        "# Generate task graph\n",
        "task_graph = make_dot(output, params=dict(model.named_parameters()))\n",
        "\n",
        "# Render and save graph\n",
        "task_graph.format = \"png\"\n",
        "task_graph.render(\"task_graph\")\n",
        "\n",
        "# Display graph\n",
        "from IPython.display import Image\n",
        "Image(\"task_graph.png\")\n"
      ],
      "metadata": {
        "id": "PhhhpOSwXJzJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}